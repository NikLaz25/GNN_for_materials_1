{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation energy prediction task 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим прогноз энергии образования с помощью модели GNN. \n",
    "\n",
    "Let's prepare a prediction of the formation energy using the GNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm, os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch libraries\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as DataLoader_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch geometric libraries\n",
    "from torch_geometric.data import Data, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from models.GNN_first import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistive libraries\n",
    "plt.rcParams.update(plt.rcParamsDefault) # Сброс настроек\n",
    "import openpyxl #, работа с excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем датафрейм df_Fm-3m.xlsx\n",
    "PATH_FOR_LOAD = r'.\\data\\datafraims\\df_Fm-3m.xlsx'\n",
    "df_load = pd.read_excel(PATH_FOR_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>structure</th>\n",
       "      <th>formation_energy_per_atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>Zr1 Sn3</td>\n",
       "      <td>Full Formula (Zr1 Sn3)\\nReduced Formula: ZrSn3...</td>\n",
       "      <td>0.115889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Zr1 Zn1</td>\n",
       "      <td>Full Formula (Zr1 Zn1)\\nReduced Formula: ZrZn\\...</td>\n",
       "      <td>0.104294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     composition                                          structure  \\\n",
       "1255     Zr1 Sn3  Full Formula (Zr1 Sn3)\\nReduced Formula: ZrSn3...   \n",
       "1256     Zr1 Zn1  Full Formula (Zr1 Zn1)\\nReduced Formula: ZrZn\\...   \n",
       "\n",
       "      formation_energy_per_atom  \n",
       "1255                   0.115889  \n",
       "1256                   0.104294  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим\n",
    "df_load.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим класс для загрузки графов из соответствующей папки репозитория  \n",
    "class ProcessedDataset_new_dir(Dataset): # (наследник torch_geometric.data.Dataset)\n",
    "    '''пользовательский класс ProcessedDataset для работы с предобработанными графами кристаллических структур, \n",
    "    сохраненными в файлах .pt, и выводит статистику о данных.'''\n",
    "\n",
    "    # Инициализирует датасет:\n",
    "    # root: Путь к папке с данными, transform/pre_transform/pre_filter: Опциональные функции для преобразования данных (не используются здесь).\n",
    "    def __init__(self, root, new_dir_for_load_grahs, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.processed_new_dir = new_dir_for_load_grahs\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return os.path.join(self.root, self.processed_new_dir)  # Кастомный путь\n",
    "    \n",
    "    # Возвращает список файлов .pt в папке processed\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        file_names = []\n",
    "        for i in os.listdir(self.processed_dir):\n",
    "            if '.pt' in i:\n",
    "                file_names.append(i)\n",
    "        return file_names\n",
    "    \n",
    "\n",
    "    # Возвращает количество графов в датасете:\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    # Загружает граф по индексу idx\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем датасет\n",
    "dataset = ProcessedDataset_new_dir('./', './data/graphs_structures_Fm_3m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.ProcessedDataset_new_dir, './data/graphs_structures_Fm_3m')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим тип и как читается ссылка\n",
    "type(dataset), dataset.processed_new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: ProcessedDataset_new_dir(1257):\n",
      "===============================================================================\n",
      "Number of graphs: 1257\n",
      "Number of features: 5\n"
     ]
    }
   ],
   "source": [
    "# смотрим данные о датасете\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('=' * 79)\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test  Разбивка данных на обучающие и тестовые"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для первого варианта возьмём размер обучающей выборки 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 1068\n"
     ]
    }
   ],
   "source": [
    "# 85% of data will be presented in the training set\n",
    "train_fraction = 0.85\n",
    "\n",
    "train_set_size = round(df_load.shape[0] * train_fraction)\n",
    "print('Number of samples in the training set:', train_set_size)\n",
    "\n",
    "\n",
    "# Get indexes for train and test # Получаем индексы обучающей и тестовой выборки\n",
    "\n",
    "# Define train indices to compare differen models\n",
    "train_idxs = df_load.sample(train_set_size).index\n",
    "\n",
    "# Inverse selection of samples that are not in the train indices\n",
    "test_idxs = df_load.loc[df_load.index.difference(train_idxs)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1068\n",
      "Number of test graphs: 189\n"
     ]
    }
   ],
   "source": [
    "# получаем \n",
    "train_dataset = dataset[list(train_idxs)]\n",
    "test_dataset = dataset[list(df_load.index.difference(train_idxs))]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of data by batches Разбивка данных по пакетам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 256\n",
      "DataBatch(x=[9912, 5], y=[256, 1], pos=[9912, 3], lattice=[256, 9], edge_index=[2, 59472], batch=[9912], ptr=[257])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 256\n",
      "DataBatch(x=[9504, 5], y=[256, 1], pos=[9504, 3], lattice=[256, 9], edge_index=[2, 57024], batch=[9504], ptr=[257])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 256\n",
      "DataBatch(x=[9312, 5], y=[256, 1], pos=[9312, 3], lattice=[256, 9], edge_index=[2, 55872], batch=[9312], ptr=[257])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 256\n",
      "DataBatch(x=[9520, 5], y=[256, 1], pos=[9520, 3], lattice=[256, 9], edge_index=[2, 57120], batch=[9520], ptr=[257])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 44\n",
      "DataBatch(x=[1616, 5], y=[44, 1], pos=[1616, 3], lattice=[44, 9], edge_index=[2, 9696], batch=[1616], ptr=[45])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# разбивка по пакетам\n",
    "train_loader = DataLoader_geometric(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader_geometric(test_dataset, batch_size=228, shuffle=False)\n",
    "\n",
    "# смотрим как разбились данные в train_loader\n",
    "# we are looking at how the data is distributed in train_loader\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 189\n",
      "DataBatch(x=[7912, 5], y=[189, 1], pos=[7912, 3], lattice=[189, 9], edge_index=[2, 47472], batch=[7912], ptr=[190])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# смотрим как разбились данные в test_loader\n",
    "# we are looking at how the data is distributed in test_loader\n",
    "for step, data in enumerate(test_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating the model / Инициируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GCN.__init__() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_embeding\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m128\u001b[39m,}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# инициируем модель \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGCN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: GCN.__init__() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters / определяем гиперпараметры \n",
    "hyperparameters = {'hidden_embeding':128,}\n",
    "\n",
    "# инициируем модель \n",
    "model = GCN(hyperparameters=hyperparameters,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор и функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание оптимизатора Adam для обучения нейронной сети. Оптимизатор отвечает за обновление весов модели в процессе обучения.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавил самостоятельно, насколько это тут необходимо пока не совсем понимаю\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция запуска подели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Импорт метрик для оценки качества модели\n",
    "# from sklearn.metrics import r2_score, mean_absolute_error\n",
    "# import numpy as np\n",
    "\n",
    "def run_model(model, epochs, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Основная функция для обучения и оценки модели.\n",
    "    \n",
    "    Параметры:\n",
    "        model (torch.nn.Module): Нейронная сеть для обучения\n",
    "        epochs (int): Количество эпох обучения\n",
    "        train_loader (torch_geometric.loader.DataLoader): Загрузчик тренировочных данных\n",
    "        test_loader (torch_geometric.loader.DataLoader): Загрузчик тестовых данных\n",
    "    \"\"\"\n",
    "    \n",
    "    def train():\n",
    "        \"\"\"\n",
    "        Внутренняя функция для одной эпохи обучения.\n",
    "        Возвращает суммарный лосс за эпоху.\n",
    "        \"\"\"\n",
    "        # Переводим модель в режим обучения (важно для слоёв типа Dropout, BatchNorm)\n",
    "        model.train()\n",
    "        val_loss = 0  # Инициализируем переменную для накопления лосса\n",
    "\n",
    "        # Итерируемся по батчам тренировочных данных\n",
    "        for data in train_loader:\n",
    "            # 1. Прямой проход (forward pass) - вычисление предсказаний модели\n",
    "            out = model(data)\n",
    "\n",
    "            # 2. Вычисление функции потерь между предсказаниями и истинными значениями\n",
    "            loss = criterion(out, data['y'])\n",
    "\n",
    "            # 3. Обратное распространение ошибки (backward pass) - вычисление градиентов\n",
    "            loss.backward()\n",
    "\n",
    "            # 4. Обновление параметров модели на основе вычисленных градиентов\n",
    "            optimizer.step()\n",
    "\n",
    "            # 5. Обнуление градиентов перед следующим батчем\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 6. Накопление лосса (detach() чтобы избежать накопления в вычислительном графе)\n",
    "            val_loss += loss.detach().item()\n",
    "\n",
    "        return val_loss  # Возвращаем суммарный лосс за эпоху\n",
    "\n",
    "\n",
    "    def test(loader):\n",
    "        \"\"\"\n",
    "        Внутренняя функция для оценки модели на переданном загрузчике данных.\n",
    "        Возвращает среднее значение R2-score по всем батчам.\n",
    "        \"\"\"\n",
    "        # Переводим модель в режим оценки (отключаем Dropout и т.д.)\n",
    "        model.eval()\n",
    "        r2 = []  # Список для хранения R2-score по каждому батчу\n",
    "\n",
    "        # Итерируемся по батчам данных (без вычисления градиентов)\n",
    "        for data in loader:\n",
    "            # Получаем предсказания модели\n",
    "            out = model(data)\n",
    "\n",
    "            # Преобразуем предсказания и истинные значения в 1D массивы\n",
    "            pred = out.detach().ravel()  # detach() чтобы не вычислять градиенты\n",
    "            true = data['y'].ravel()\n",
    "\n",
    "            # Вычисляем R2-score для текущего батча и сохраняем\n",
    "            r2.append(r2_score(pred.numpy(), true.numpy()))\n",
    "\n",
    "        # Возвращаем среднее значение R2-score по всем батчам\n",
    "        return np.array(r2).mean()\n",
    "\n",
    "    # Основной цикл обучения\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Одна эпоха обучения и получение лосса\n",
    "        val_loss = train()\n",
    "        \n",
    "        # Вывод информации о текущей эпохе\n",
    "        print(f'Epoch: {epoch:03d}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Каждые 10 эпох оцениваем модель на тренировочных и тестовых данных\n",
    "        if epoch % 10 == 0:\n",
    "            train_acc = test(train_loader)  # R2 на тренировочных данных\n",
    "            test_acc = test(test_loader)   # R2 на тестовых данных\n",
    "            print(f'Train R2: {train_acc:.4f}, Test R2: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_materials_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
